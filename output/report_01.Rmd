---
title: "Predicting Monarchs in Mexico with Machine Learning Models"
author: "Keaton Wilson"
date: "7/2/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "~/Documents/Projects/insect_migration/")
```

# Introduction  
The basic goal of this project is to generate a Machine Learning model (or models) that predicts hectares in Mexico of Eastern Monarchs (a problematic, but common measure of population success) using citizen-science data and publicly available environmental data. A few challenges:  
1. There are only ~ 24-25 years of data.  
2. What features/variables do we use?  

### ML with Small Data  

The strategy here is to build a full feature set for our small "real" data, and then use the `synthpop` package to generate a larger data set (~25k observations) of synthetic data based on the properties of the original data set. Then, we'll train ML models on the synthetic data and use the 'real' data to evaluate.  

### Appropriate Feature Building  
The other tricky thing here is building the appropriate variables. Most of this comes out of a [blog post by Chip Taylor](https://monarchwatch.org/blog/2019/05/02/monarch-population-status-39/).  

The features I used here are:  
1. The date of first sighting in the US  
2. Number of sightings above 37ยบ latitude  
3. Total number of sightings between March and October  
4. Environmental conditions (19 bioclim variables) of the entire eastern corridor  
5. Environmental conditions (19 bioclim variables) of the northern part of the territory (above 37ยบ, that will end up creating 'migratory' morphs)  

## Data Collection and Feature Generation  
### Data Collection  

I used the custom `get_clean_obs` function (a wrapper for spocc) that pulls all (or most, depending on the number of records) of records from GBIF and inat.  After some cleaning, filtering and reverse geocoding, we get the table below.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
monarch = read_csv("./data/monarch_w_locs.csv")
glimpse(read_csv("./data/monarch_w_locs.csv"))

```

This is a substantial data set, and we can plot it to look to see if the records make sense.  

```{r echo=FALSE, message=FALSE, warning = FALSE}
library(ggmap)
library(mapr)
register_google(key = "AIzaSyDyAqUc4o9p_DOBSF_JOXH5c_JXPqoU4Yw")

america = get_map(location = "Kansas", source = "stamen", zoom = 3)
ggmap(america) +
  geom_point(data = monarch, aes(x = longitude, y = latitude), size = 0.5, alpha = 0.2)
```

Ok, these look reasonable. I've taken additional steps not outlined here to do a bit of cleaning on the data before we start building features below. 
 
```{r, echo=FALSE, warning=FALSE, message=FALSE, results=FALSE}
#First step is setting up the dataframe that all of this will populate
#Reading in monarch winter data
monarch_winter = read_csv("./data/monarch_winter.csv")
monarch_ml_df = monarch_winter

#inspecting
print(monarch_ml_df, n=30)

#Changing dates to December dates - i.e. the year reflects the year in December, not in January when the records were collected. 
monarch_ml_df$year = monarch_ml_df$year - 1

#extracting location info and cleaning up
monarch$country = str_extract(monarch$locs, '\\b[^,]+$')
unique(monarch$country)

na_country_list = c("USA", "Mexico", "Puerto Rico", "Canada", "United States")

monarch_country_sub = monarch %>%
  filter(country %in% na_country_list) %>%
  mutate(country = str_replace(country, "United States", "USA"), 
         country = str_replace(country, "Peurto Rico", "USA"))
```

### Feature 1. Date of first sighting
```{r, message=FALSE, warning=FALSE}
library(lubridate)
#Building a dataframe of first sightings
first_us_sighting = monarch_country_sub %>%
  mutate(year = year(date)) %>% 
  filter(country == "USA") %>%
  filter(str_detect(locs, "TX|Texas|NM|New Mexico|OK|Oklahoma|AZ|Arizona")) %>% #Restricting to states that make sense
  group_by(year) %>%
  summarize(first_sighting = min(date), 
            n = n()) %>%
  print(n = 50)

```
So, they're really all over the place, but they appear to be getting sooner as time goes on? Interesting. Regardless, we're going to bind them to the master data frame  

```{r}
#binding
monarch_ml_df = monarch_ml_df %>%
  left_join(first_us_sighting, by = "year") %>%
  mutate(day_first_sighting = lubridate::yday((first_sighting))) %>%
  dplyr::select(-first_sighting)
```

### Feature 2. Number of sightings north of 37ยบ  
Adding a feature that is the number of sightings north of 37ยบ divided by/normalized by the total number of sightings for a year  

```{r}
monarch_ml_df = monarch %>%
  mutate(year = year(date)) %>%
  group_by(year) %>%
  summarize(n_obs_total = n()) %>%
  right_join(monarch_ml_df, by = "year") %>%
  left_join(monarch %>%
               mutate(year = year(date)) %>%
               filter(latitude > 37) %>%
               group_by(year) %>%
               summarize(n_obs_37 = n()), by = "year") %>%
  mutate(obs_37_norm = n_obs_37/n_obs_total) %>%
  dplyr::select(year, hectares, day_first_sighting, obs_37_norm) %>%
  print(n = 50)
```

### Feature 3. Number of sightings during the active season (March-October)  
Adding a feature that is the number of sightings during the active season divided by/normalied by the total number of sightings for a year. Also going to leave on the total number of observations by year. 

```{r}
monarch_ml_df = monarch %>%
  mutate(year = year(date), 
         month = month(date)) %>%
  filter(month %in% 3:10) %>%
  group_by(year) %>%
  summarize(active_months_obs = n()) %>% 
  right_join(monarch_ml_df, by = "year") %>%
  left_join(monarch %>%
              mutate(year = year(date)) %>%
              group_by(year) %>%
              summarize(n_obs_total = n()), by = "year") %>%
  mutate(active_months_obs_norm = active_months_obs/n_obs_total) %>%
  dplyr::select(year, hectares, day_first_sighting, obs_37_norm, active_months_obs_norm, n_obs_total) %>%
  print(n = 50)
```

### Features 4-5. Environmental Variables   
Adding a bunch of features. The idea here is to generate summaries of bioclim variables (of which there are 19) for two geographic regions. The first is the whole eastern chunk, and the second is the northern range, or the part of range that generates migratory morphs, boundary boxes shown below. 

```{r, echo=FALSE}
#Cropping - Feature #1 Total Area
lon_min = -110
lon_max = -80
lat_min = 25
lat_max = 50

#feature 2 area
lon_min_2 = -110
lon_max_2 = -80
lat_min_2 = 37
lat_max_2 = 50

ggmap(america) +
  geom_rect(aes(xmin = lon_min, ymin = lat_min, xmax = lon_max, ymax = lat_max),
             color = "red", alpha = 0.1) +
  geom_rect(aes(xmin = lon_min_2, ymin = lat_min_2, xmax = lon_max_2, ymax = lat_max_2),
             color = "blue", alpha = 0.1, lty = 2) +
  annotate("text", x = -105, y = 51, label = "Northern Boundary Box", size = 2, hjust = 0, color = "blue") +
  annotate("text", x = -75, y = 38, label = "Eastern Habitat Boundary Box", size = 2, hjust = 0, color = "red")
```

A lot of prism environmental data wrangling that I'm not going to show here, but the big thing is averaging bioclim cells for each area for each year. Also, I added the previous year's count (under the assumption that if you know something about last year's numbers, it might be a good predictor for this year's numbers).  

So let's take a look at the final version of the 'real' data. 

```{r, echo = FALSE, warning = FALSE, message=FALSE}
monarch_ml_df = read_csv("./data/monarch_data_real.csv")
```

```{r}
glimpse(monarch_ml_df)
```

So, we've got a really big feature set (43 variables!). For information on what each bioclim variable represents, check out [this link](https://www.worldclim.org/bioclim).  

## Synthetic data generation  
Here, I use the `synthpop` package to generate 25k observations based on the original data set. `synthpop` uses classification and regression trees (though you can use other predictive algorithms). Below, I'll demonstrate the code, and also look at some comparisons between the real data dn the synthetic data. 

```{r, warning=FALSE, message=FALSE}
library(synthpop)

#reading in the 'real' data
monarch_data =read_csv("./data/monarch_data_real.csv")

#Generating a list for all the variables indicating that we want to use density smoothing
smooth_list = as.list(rep("density", 45))
names(smooth_list) = names(monarch_data)

#Generating synthetic data
syn_monarch = synthpop::syn(monarch_data, k = 25000, smoothing = smooth_list)

glimpse(syn_monarch$syn)

#Plotting
ggplot(data = syn_monarch$syn, aes(x = hectares)) +
  geom_density(fill = "red", alpha = 0.5) +
  geom_density(data = monarch_data, aes(x = hectares), fill = "blue", alpha = 0.5) +
  theme_classic()
```
So this is interesting - it looks like the synthetic data generation is matching the original data set very closely but magnifying small-scale structure. In the above graph blue is the original kernel density estimate and red is the synthetic data. Need to look at ways to smooth the curve so it more closely matches the original. Perhaps another algorithm that isn't quite as good at predicition would do the trick.  


## Preprocessing the data prior to algorithm building  
We're going to use the `recipes` package to build a pre-processing recipe that we'll then apply to the training (synthetic) and test (real) sets before we start algorithm building. The main preprocessing steps we want to accomplish are:  
1. **k-nn imputation to fill in any missing values**  
2. **Center and scale the data**  
3. **Remove any variables where there is zero or near-zero variance**  
4. **Also tried some PCA on bioclim variables, but resulted in large drops in predictive power...ended up removing this**  

## Algorithm building  

I built and tuned four algorithms with 5-fold cross validation with 5 repeats:  
1. **Linear regression**  
2. **Random Forest**  
3. **Extreme Gradient Boosting**  
4. **Ridge and Lasso Regression**  

```{r, echo = FALSE, message=FALSE, warning = FALSE}
#Reading in saved model objects
library(caret)
lm_mod = readRDS("./output/lm_model.rds")
rf_mod = readRDS("./output/rf_model.rds")
ridge_lasso_mod = readRDS("./output/ridge_lasso_mod.rds")
xgboost_mod = readRDS("./output/xgboost_model.rds")
```

```{r}
#Let's compare models

#Training data evaluation
results <- resamples(list(RandomForest=rf_mod, linearreg=lm_mod, xgboost = xgboost_mod, ridge_lass = ridge_lasso_mod))

# summarize the distributions
summary(results)

# boxplot of results
bwplot(results, metric="RMSE")
bwplot(results, metric="MAE")

#Test Data evaluation
monarch_test_data = read_csv("./data/monarch_test_data.csv")
#Making predictions
rf_mod_fit = predict(rf_mod, monarch_test_data)
xgboost_mod_fit = predict(xgboost_mod, monarch_test_data)
lm_mod_fit = predict(lm_mod, monarch_test_data)
ridge_lasso_fit = predict(ridge_lasso_mod, monarch_test_data)

#Post resample on test data
postResample(pred = rf_mod_fit, obs = monarch_test_data$hectares)
postResample(pred = xgboost_mod_fit, obs = monarch_test_data$hectares)
postResample(pred = lm_mod_fit, obs = monarch_test_data$hectares)
postResample(pred = ridge_lasso_fit, obs = monarch_test_data$hectares)

```

So, overall these are pretty decent as a first pass. The best model, the extreme gradient boosting model is explaining **over 71%** of the variance in wintering numbers and has a mean absolute errors of about 1.65 hectares.  
We can also visualize the predictions against the real results for a bit more insight.  

```{r}
#binding on predicitions
monarch_test_data$pred = xgboost_mod_fit

ggplot(monarch_test_data, aes(x = hectares, y = pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, lty = 2) +
  theme_classic() +
  xlab("Actual Hectares") +
  ylab("Predicted Hectares") +
  xlim(c(0,25)) +
  ylim(c(0,15))

```

Overall, this is a pretty good fit - though on average it looks like low values are consistently over predicted and there is one high value that is *WAY* under-predicted. The dashed line above is the 1:1 line - representing perfect prediction.

We can also try and try and glimpse inside the black box by looking at variable importance.
```{r}
varImp(xgboost_mod)
```

This is super interesting. Bioclim 8, 14, 19 are the most importance variables, which are mean temperature of the wettest quarter, precipitation of the driest month and precipitation of the coldest quarter, respectively. Interesting to think about why these might pop out and how they're related to monarch biology.   

Other important factors include total number of observations, and day of first sighting. Number of observations is problematic, because it's not scaled for effort - but may be correlated with a strong downward trend in population numbers (i.e. more recent records always have more total observations because of high effort, but more recent records also have lower numbers because of declines). Need to figure out a good way to control for this in this context.  

## Future Directions  

This is a first pass. I think we can improve the predictive power of the model in a number of ways.  

1. **Larger synthetic data set.** These were built/trained with 25k records - we can definitely increase this. Something like 100k or 500k would be great, but it's going to require substantial computing power on Cyverse.  

2. **Adaptive Tuning.** The `caret` package supports adaptive tuning where once we have a set of model parameters that are producing good results we can narrow the search grid to fine tune these parameters.  

3. **Ensemble models.** I don't show it hear, but the models have fairly low correlations among each other, making them a good fit for ensemble methods. This has worked in a past project I did to increase accuracy.  

4. **Fine tuning the input variables.**. There are probably other variables that we could think of that are important and we might be able to add. Adding more important biological features would probably not be a bad idea.  

